{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthKulkarni445/SigmaStox/blob/main/Portfolio_Implied_Volatility.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utilities**"
      ],
      "metadata": {
        "id": "wVuOWASLNs5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "import warnings\n",
        "from scipy.stats import pearsonr\n",
        "from itertools import combinations\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    \"\"\"Custom Dataset class for stock price sequences\"\"\"\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor([self.targets[idx]])\n",
        "\n",
        "class PortfolioDataset(Dataset):\n",
        "    \"\"\"Dataset for portfolio-level predictions\"\"\"\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.features[idx]), torch.FloatTensor([self.targets[idx]])\n",
        "\n",
        "def calculate_realized_volatility(prices, window=20):\n",
        "    \"\"\"Calculate realized volatility using rolling window\"\"\"\n",
        "    returns = np.log(prices / prices.shift(1)).dropna()\n",
        "    realized_vol = returns.rolling(window=window).std() * np.sqrt(252)  # Annualized\n",
        "    return realized_vol\n",
        "\n",
        "def create_stock_features(data, lookback_window=60):\n",
        "    \"\"\"Create features from individual stock data\"\"\"\n",
        "    data['Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "    data['High_Low_Pct'] = (data['High'] - data['Low']) / data['Close']\n",
        "    data['Open_Close_Pct'] = (data['Close'] - data['Open']) / data['Open']\n",
        "\n",
        "    data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
        "    data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
        "    data['MA_Ratio'] = data['MA_5'] / data['MA_20']\n",
        "\n",
        "    data['Realized_Vol'] = calculate_realized_volatility(data['Close'])\n",
        "    data['Vol_MA'] = data['Realized_Vol'].rolling(window=10).mean()\n",
        "\n",
        "    data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "    volume_series = pd.Series(data['Volume'].values.flatten(), index=data.index)\n",
        "    data['Volume_Ratio'] = (volume_series / data['Volume_MA']).fillna(1.0)\n",
        "\n",
        "    delta = data['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    data['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    feature_columns = [\n",
        "        'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "        'Returns', 'High_Low_Pct', 'Open_Close_Pct',\n",
        "        'MA_Ratio', 'Volume_Ratio', 'RSI'\n",
        "    ]\n",
        "\n",
        "    data = data.replace([np.inf, -np.inf], np.nan)\n",
        "    data = data.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    data['Target_IV'] = data['Realized_Vol'].shift(-5)  # 5-day ahead volatility\n",
        "\n",
        "    return data[feature_columns + ['Target_IV']].dropna()\n",
        "\n",
        "def prepare_stock_sequences(data, sequence_length=60):\n",
        "    \"\"\"Prepare sequences for LSTM training\"\"\"\n",
        "    feature_columns = [col for col in data.columns if col != 'Target_IV']\n",
        "\n",
        "    feature_scaler = MinMaxScaler()\n",
        "    target_scaler = MinMaxScaler()\n",
        "\n",
        "    scaled_features = feature_scaler.fit_transform(data[feature_columns])\n",
        "    scaled_targets = target_scaler.fit_transform(data[['Target_IV']])\n",
        "\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(sequence_length, len(scaled_features)):\n",
        "        sequences.append(scaled_features[i-sequence_length:i])\n",
        "        targets.append(scaled_targets[i, 0])\n",
        "\n",
        "    return np.array(sequences), np.array(targets), feature_scaler, target_scaler\n",
        "\n",
        "def save_model(model, filepath, scalers=None, metadata=None):\n",
        "    \"\"\"Save model with scalers and metadata\"\"\"\n",
        "    save_dict = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_class': model.__class__.__name__,\n",
        "        'scalers': scalers,\n",
        "        'metadata': metadata,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    torch.save(save_dict, filepath)\n",
        "    print(f\"Model saved to {filepath}\")\n",
        "\n",
        "def load_model(filepath, model_class):\n",
        "    \"\"\"Load model with scalers and metadata\"\"\"\n",
        "    checkpoint = torch.load(filepath)\n",
        "\n",
        "    # You'll need to instantiate the model with the correct parameters\n",
        "    # This is a simplified version - you might need to save model parameters too\n",
        "    model = model_class()\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    return model, checkpoint.get('scalers'), checkpoint.get('metadata')"
      ],
      "metadata": {
        "id": "RTBZKRM3L541"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage I : Stock IV Forecasting**"
      ],
      "metadata": {
        "id": "nWg_TEqZN0_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"LSTM Model for Individual Stock IV Prediction\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "\n",
        "        normalized = self.batch_norm(last_output)\n",
        "\n",
        "        out = self.relu(self.fc1(normalized))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def train_lstm_model(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
        "    \"\"\"Train individual stock LSTM model\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 20\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for sequences, targets in train_loader:\n",
        "            sequences, targets = sequences.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, targets in val_loader:\n",
        "                sequences, targets = sequences.to(device), targets.to(device)\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "    return train_losses, val_losses, best_val_loss\n",
        "\n",
        "def optimize_lstm_hyperparameters(X_train, y_train, X_val, y_val, input_size, device, n_trials=50):\n",
        "    \"\"\"Optimize LSTM hyperparameters using Optuna\"\"\"\n",
        "\n",
        "    def objective(trial):\n",
        "        hidden_size = trial.suggest_categorical('hidden_size', [64, 128])\n",
        "        num_layers = trial.suggest_int('num_layers', 1, 2)\n",
        "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "\n",
        "        model = LSTMModel(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout_rate=dropout_rate\n",
        "        ).to(device)\n",
        "\n",
        "        train_dataset = StockDataset(X_train, y_train)\n",
        "        val_dataset = StockDataset(X_val, y_val)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        _, _, best_val_loss = train_lstm_model(\n",
        "            model, train_loader, val_loader, 50, learning_rate, device\n",
        "        )\n",
        "\n",
        "        return best_val_loss\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for key, value in study.best_params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    return study.best_params"
      ],
      "metadata": {
        "id": "tSDSsI_ONYxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Portfolio Features**"
      ],
      "metadata": {
        "id": "N2qzCsZdOZRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def calculate_correlation_matrix(returns_data, window=60):\n",
        "    \"\"\"Calculate rolling correlation matrix between stocks\"\"\"\n",
        "    correlations = []\n",
        "    correlation_dates = []\n",
        "\n",
        "    for i in range(window, len(returns_data)):\n",
        "        window_data = returns_data.iloc[i-window:i]\n",
        "        corr_matrix = window_data.corr().values\n",
        "        corr_matrix = np.nan_to_num(corr_matrix)\n",
        "        correlations.append(corr_matrix)\n",
        "        correlation_dates.append(returns_data.index[i])\n",
        "\n",
        "    return np.array(correlations), correlation_dates\n",
        "\n",
        "def create_portfolio_features(individual_ivs, weights, correlations, correlation_dates, market_data):\n",
        "    \"\"\"\n",
        "    Create portfolio-level features for Stage II\n",
        "    Args:\n",
        "        individual_ivs: Dict of individual stock IV predictions\n",
        "        weights: Portfolio weights for each stock\n",
        "        correlations: Correlation matrices over time\n",
        "        correlation_dates: Dates corresponding to correlation matrices\n",
        "        market_data: Market-wide indicators (VIX, etc.)\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    feature_dates = []\n",
        "\n",
        "    aligned_dates = None\n",
        "    for ticker, iv_series in individual_ivs.items():\n",
        "        if aligned_dates is None:\n",
        "            aligned_dates = iv_series.index\n",
        "        else:\n",
        "            aligned_dates = aligned_dates.intersection(iv_series.index)\n",
        "\n",
        "    aligned_dates = aligned_dates.sort_values()\n",
        "    correlation_dates_index = pd.Index(correlation_dates)\n",
        "\n",
        "    common_dates = aligned_dates.intersection(correlation_dates_index)\n",
        "\n",
        "    for date in common_dates:\n",
        "        feature_vector = []\n",
        "\n",
        "        stock_ivs = []\n",
        "        for ticker in individual_ivs.keys():\n",
        "            if date in individual_ivs[ticker].index:\n",
        "                stock_ivs.append(individual_ivs[ticker].loc[date])\n",
        "            else:\n",
        "                if not individual_ivs[ticker].empty:\n",
        "                    stock_ivs.append(individual_ivs[ticker].iloc[-1])\n",
        "                else:\n",
        "                    stock_ivs.append(0)\n",
        "\n",
        "        feature_vector.extend(stock_ivs)\n",
        "\n",
        "        feature_vector.extend(weights[:len(stock_ivs)])\n",
        "\n",
        "        if date in correlation_dates_index:\n",
        "            corr_index = correlation_dates_index.get_loc(date)\n",
        "            corr_matrix = correlations[corr_index]\n",
        "\n",
        "            if corr_matrix.shape[0] == len(stock_ivs):\n",
        "                upper_tri_indices = np.triu_indices_from(corr_matrix, k=1)\n",
        "                if len(upper_tri_indices[0]) > 0:\n",
        "                    avg_correlation = np.mean(corr_matrix[upper_tri_indices])\n",
        "                    max_correlation = np.max(corr_matrix[upper_tri_indices])\n",
        "                    min_correlation = np.min(corr_matrix[upper_tri_indices])\n",
        "                    feature_vector.extend([avg_correlation, max_correlation, min_correlation])\n",
        "                else:\n",
        "                    feature_vector.extend([0, 0, 0])\n",
        "            else:\n",
        "                feature_vector.extend([np.nan, np.nan, np.nan])\n",
        "        else:\n",
        "            feature_vector.extend([np.nan, np.nan, np.nan])\n",
        "\n",
        "        if market_data is not None and date in market_data.index:\n",
        "            if 'VIX' in market_data.columns:\n",
        "                vix_val = market_data.loc[date, 'VIX']\n",
        "                feature_vector.append(vix_val if not np.isnan(vix_val) else 0)\n",
        "            else:\n",
        "                feature_vector.append(0)\n",
        "\n",
        "            if 'Market_Vol' in market_data.columns:\n",
        "                market_vol = market_data.loc[date, 'Market_Vol']\n",
        "                feature_vector.append(market_vol if not np.isnan(market_vol) else 0)\n",
        "            else:\n",
        "                feature_vector.append(0)\n",
        "        else:\n",
        "            feature_vector.extend([0, 0])\n",
        "\n",
        "        if len(stock_ivs) == len(weights[:len(stock_ivs)]):\n",
        "            weighted_iv = np.sum(np.array(stock_ivs) * np.array(weights[:len(stock_ivs)]))\n",
        "            iv_dispersion = np.std(stock_ivs) if len(stock_ivs) > 1 else 0\n",
        "            feature_vector.extend([weighted_iv, iv_dispersion])\n",
        "        else:\n",
        "            feature_vector.extend([np.nan, np.nan])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "        feature_dates.append(date)\n",
        "\n",
        "    features_df = pd.DataFrame(features, index=feature_dates)\n",
        "    features_df = features_df.fillna(0)\n",
        "\n",
        "    return features_df.values, features_df.index\n",
        "\n",
        "def calculate_portfolio_iv_target(individual_ivs, weights, correlations, correlation_dates):\n",
        "    \"\"\"\n",
        "    Calculate target portfolio IV using correlation-adjusted formula\n",
        "    Portfolio Variance = w^T * Σ * w\n",
        "    where Σ is the covariance matrix\n",
        "    \"\"\"\n",
        "    targets = []\n",
        "    target_dates = []\n",
        "\n",
        "    aligned_dates = None\n",
        "    for ticker, iv_series in individual_ivs.items():\n",
        "        if aligned_dates is None:\n",
        "            aligned_dates = iv_series.index\n",
        "        else:\n",
        "            aligned_dates = aligned_dates.intersection(iv_series.index)\n",
        "\n",
        "    aligned_dates = aligned_dates.sort_values()\n",
        "    correlation_dates_index = pd.Index(correlation_dates)\n",
        "    common_dates = aligned_dates.intersection(correlation_dates_index)\n",
        "\n",
        "    for date in common_dates:\n",
        "        ivs = []\n",
        "        for ticker in individual_ivs.keys():\n",
        "            if date in individual_ivs[ticker].index:\n",
        "                ivs.append(individual_ivs[ticker].loc[date])\n",
        "            else:\n",
        "                if not individual_ivs[ticker].empty:\n",
        "                    ivs.append(individual_ivs[ticker].iloc[-1])\n",
        "                else:\n",
        "                    ivs.append(0)\n",
        "\n",
        "        ivs = np.array(ivs)\n",
        "        weights_array = np.array(weights[:len(ivs)])\n",
        "\n",
        "        if date in correlation_dates_index:\n",
        "            corr_index = correlation_dates_index.get_loc(date)\n",
        "            corr_matrix = correlations[corr_index]\n",
        "\n",
        "            if corr_matrix.shape[0] == len(ivs):\n",
        "                cov_matrix = np.outer(ivs, ivs) * corr_matrix\n",
        "\n",
        "                portfolio_var = np.dot(weights_array, np.dot(cov_matrix, weights_array))\n",
        "                portfolio_iv = np.sqrt(max(portfolio_var, 0))\n",
        "\n",
        "                targets.append(portfolio_iv)\n",
        "                target_dates.append(date)\n",
        "\n",
        "    return np.array(targets), target_dates"
      ],
      "metadata": {
        "id": "8HVJnlLhOWrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 2 : Portfolio IV Prediction**"
      ],
      "metadata": {
        "id": "OSjiRU_8OGiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "class PortfolioMLPModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron for Portfolio IV Prediction\n",
        "    Input Features:\n",
        "    - Individual stock IV predictions\n",
        "    - Portfolio weights\n",
        "    - Pairwise correlations\n",
        "    - Market regime indicators\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_sizes=[128, 64, 32], dropout_rate=0.3):\n",
        "        super(PortfolioMLPModel, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_size, hidden_size),\n",
        "                nn.BatchNorm1d(hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class PortfolioGNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Neural Network for Portfolio IV Prediction\n",
        "    Models stock relationships through correlation graph\n",
        "    \"\"\"\n",
        "    def __init__(self, num_stocks, feature_dim, hidden_dim=64):\n",
        "        super(PortfolioGNNModel, self).__init__()\n",
        "        self.num_stocks = num_stocks\n",
        "        self.feature_dim = feature_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.node_transform = nn.Linear(feature_dim, hidden_dim)\n",
        "\n",
        "        self.gconv1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.gconv2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.portfolio_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_features, adjacency_matrix, weights):\n",
        "        h = torch.relu(self.node_transform(node_features))\n",
        "\n",
        "        h = torch.relu(self.gconv1(torch.matmul(adjacency_matrix, h)))\n",
        "        h = torch.relu(self.gconv2(torch.matmul(adjacency_matrix, h)))\n",
        "\n",
        "        portfolio_embedding = torch.sum(h * weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        output = self.portfolio_fc(portfolio_embedding)\n",
        "        return output\n",
        "\n",
        "def train_portfolio_model(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
        "    \"\"\"Train portfolio-level model\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=15, factor=0.5)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 25\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for features, targets in train_loader:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, targets in val_loader:\n",
        "                features, targets = features.to(device), targets.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "    return train_losses, val_losses, best_val_loss\n",
        "\n",
        "def optimize_mlp_hyperparameters(X_train, y_train, X_val, y_val, device, n_trials=30):\n",
        "    \"\"\"Optimize MLP hyperparameters using Optuna\"\"\"\n",
        "\n",
        "    def objective(trial):\n",
        "        hidden_sizes = []\n",
        "        n_layers = trial.suggest_int('n_layers', 2, 5)\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            hidden_size = trial.suggest_categorical(f'hidden_size_{i}', [32, 64, 128, 256, 512])\n",
        "            hidden_sizes.append(hidden_size)\n",
        "\n",
        "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
        "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "\n",
        "        model = PortfolioMLPModel(\n",
        "            input_size=X_train.shape[1],\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            dropout_rate=dropout_rate\n",
        "        ).to(device)\n",
        "\n",
        "        train_dataset = PortfolioDataset(X_train, y_train)\n",
        "        val_dataset = PortfolioDataset(X_val, y_val)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        _, _, best_val_loss = train_portfolio_model(\n",
        "            model, train_loader, val_loader, 100, learning_rate, device\n",
        "        )\n",
        "\n",
        "        return best_val_loss\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "    print(\"Best MLP hyperparameters:\")\n",
        "    for key, value in study.best_params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    return study.best_params"
      ],
      "metadata": {
        "id": "qPRuYlmBNodA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting Utilities**"
      ],
      "metadata": {
        "id": "dsd-H86nOyQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def plot_stage1_results(train_losses, val_losses, y_true, y_pred, ticker):\n",
        "    \"\"\"Plot Stage I (individual stock) results\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(f'Stage I Results - {ticker}', fontsize=16)\n",
        "\n",
        "    axes[0, 0].plot(train_losses, label='Training Loss', alpha=0.8)\n",
        "    axes[0, 0].plot(val_losses, label='Validation Loss', alpha=0.8)\n",
        "    axes[0, 0].set_title('Training History')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss (MSE)')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].scatter(y_true, y_pred, alpha=0.6)\n",
        "    axes[0, 1].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "    axes[0, 1].set_xlabel('Actual IV')\n",
        "    axes[0, 1].set_ylabel('Predicted IV')\n",
        "    axes[0, 1].set_title('Actual vs Predicted')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 2].plot(y_true[:200], label='Actual', alpha=0.8)\n",
        "    axes[0, 2].plot(y_pred[:200], label='Predicted', alpha=0.8)\n",
        "    axes[0, 2].set_title('Time Series (First 200 points)')\n",
        "    axes[0, 2].set_xlabel('Time')\n",
        "    axes[0, 2].set_ylabel('IV')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    residuals = y_true - y_pred\n",
        "    axes[1, 0].scatter(y_pred, residuals, alpha=0.6)\n",
        "    axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[1, 0].set_xlabel('Predicted IV')\n",
        "    axes[1, 0].set_ylabel('Residuals')\n",
        "    axes[1, 0].set_title('Residuals Plot')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('Residuals')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('Residuals Distribution')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 2].plot(np.abs(residuals), alpha=0.7)\n",
        "    axes[1, 2].set_xlabel('Time')\n",
        "    axes[1, 2].set_ylabel('Absolute Error')\n",
        "    axes[1, 2].set_title('Absolute Error Over Time')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_stage2_results(train_losses, val_losses, y_true, y_pred, model_name):\n",
        "    \"\"\"Plot Stage II (portfolio) results\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(f'Stage II Results - {model_name}', fontsize=16)\n",
        "\n",
        "    axes[0, 0].plot(train_losses, label='Training Loss', alpha=0.8)\n",
        "    axes[0, 0].plot(val_losses, label='Validation Loss', alpha=0.8)\n",
        "    axes[0, 0].set_title('Training History')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss (MSE)')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].scatter(y_true, y_pred, alpha=0.6)\n",
        "    axes[0, 1].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "    axes[0, 1].set_xlabel('Actual Portfolio IV')\n",
        "    axes[0, 1].set_ylabel('Predicted Portfolio IV')\n",
        "    axes[0, 1].set_title('Actual vs Predicted')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 2].plot(y_true, label='Actual', alpha=0.8)\n",
        "    axes[0, 2].plot(y_pred, label='Predicted', alpha=0.8)\n",
        "    axes[0, 2].set_title('Portfolio IV Time Series')\n",
        "    axes[0, 2].set_xlabel('Time')\n",
        "    axes[0, 2].set_ylabel('Portfolio IV')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    residuals = y_true - y_pred\n",
        "    axes[1, 0].scatter(y_pred, residuals, alpha=0.6)\n",
        "    axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[1, 0].set_xlabel('Predicted Portfolio IV')\n",
        "    axes[1, 0].set_ylabel('Residuals')\n",
        "    axes[1, 0].set_title('Residuals Plot')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('Residuals')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('Residuals Distribution')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    correlation, p_value = pearsonr(y_true, y_pred)\n",
        "    axes[1, 2].text(0.1, 0.9, f'Correlation: {correlation:.4f}', transform=axes[1, 2].transAxes, fontsize=12)\n",
        "    axes[1, 2].text(0.1, 0.8, f'P-value: {p_value:.4f}', transform=axes[1, 2].transAxes, fontsize=12)\n",
        "    axes[1, 2].text(0.1, 0.7, f'RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.4f}', transform=axes[1, 2].transAxes, fontsize=12)\n",
        "    axes[1, 2].text(0.1, 0.6, f'MAE: {mean_absolute_error(y_true, y_pred):.4f}', transform=axes[1, 2].transAxes, fontsize=12)\n",
        "    axes[1, 2].set_title('Model Performance Metrics')\n",
        "    axes[1, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_portfolio_comparison(results_dict):\n",
        "    \"\"\"Compare different portfolio models\"\"\"\n",
        "    models = list(results_dict.keys())\n",
        "    metrics = ['RMSE', 'MAE', 'Correlation']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = []\n",
        "        for model in models:\n",
        "            if metric == 'RMSE':\n",
        "                values.append(results_dict[model]['rmse'])\n",
        "            elif metric == 'MAE':\n",
        "                values.append(results_dict[model]['mae'])\n",
        "            elif metric == 'Correlation':\n",
        "                values.append(results_dict[model]['correlation'])\n",
        "\n",
        "        axes[i].bar(models, values, alpha=0.7)\n",
        "        axes[i].set_title(f'{metric} Comparison')\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LD4Mm233OvLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage I Exexcution**"
      ],
      "metadata": {
        "id": "18yOvcQyOkiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1: Individual Stock IV Prediction\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "tickers = [\n",
        "    \"MSFT\", \"NVDA\", \"AAPL\", \"AMZN\", \"META\", \"AVGO\", \"TSLA\", \"GOOGL\",\n",
        "    \"HD\", \"GOOG\", \"JPM\", \"WMT\", \"LLY\", \"V\", \"ORCL\", \"NFLX\",\n",
        "    \"XOM\", \"MA\", \"COST\", \"PG\"\n",
        "]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create directories for saving models\n",
        "os.makedirs('models/stage1', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STAGE I: INDIVIDUAL STOCK IV PREDICTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "stage1_results = {}\n",
        "individual_iv_predictions = {}\n",
        "stock_returns_data = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    print(f\"\\nProcessing {ticker}...\")\n",
        "\n",
        "    try:\n",
        "        stock_data = yf.download(ticker, start='2020-01-01', end='2024-01-01')\n",
        "\n",
        "        if len(stock_data) < 200:\n",
        "            print(f\"Insufficient data for {ticker}\")\n",
        "            continue\n",
        "\n",
        "        returns = np.log(stock_data['Close'] / stock_data['Close'].shift(1)).dropna()\n",
        "        stock_returns_data[ticker] = returns\n",
        "\n",
        "        processed_data = create_stock_features(stock_data)\n",
        "\n",
        "        X, y, feature_scaler, target_scaler = prepare_stock_sequences(processed_data)\n",
        "\n",
        "        train_size = int(0.7 * len(X))\n",
        "        val_size = int(0.15 * len(X))\n",
        "\n",
        "        X_train, y_train = X[:train_size], y[:train_size]\n",
        "        X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "        X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
        "\n",
        "        input_size = X.shape[2]\n",
        "\n",
        "        print(f\"Optimizing hyperparameters for {ticker}...\")\n",
        "        best_params = optimize_lstm_hyperparameters(\n",
        "            X_train, y_train, X_val, y_val, input_size, device, n_trials=6\n",
        "        )\n",
        "\n",
        "        best_model = LSTMModel(\n",
        "            input_size=input_size,\n",
        "            hidden_size=best_params['hidden_size'],\n",
        "            num_layers=best_params['num_layers'],\n",
        "            dropout_rate=best_params['dropout_rate']\n",
        "        ).to(device)\n",
        "\n",
        "        train_dataset = StockDataset(X_train, y_train)\n",
        "        val_dataset = StockDataset(X_val, y_val)\n",
        "        test_dataset = StockDataset(X_test, y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
        "\n",
        "        print(f\"Training final model for {ticker}...\")\n",
        "        train_losses, val_losses, _ = train_lstm_model(\n",
        "            best_model, train_loader, val_loader, 150, best_params['learning_rate'], device\n",
        "        )\n",
        "\n",
        "        model_path = f'models/stage1/{ticker}_best_model.pth'\n",
        "        save_model(\n",
        "            best_model,\n",
        "            model_path,\n",
        "            scalers=(feature_scaler, target_scaler),\n",
        "            metadata={\n",
        "                'ticker': ticker,\n",
        "                'best_params': best_params,\n",
        "                'input_size': input_size,\n",
        "                'train_size': train_size,\n",
        "                'val_size': val_size,\n",
        "                'test_size': len(X_test)\n",
        "            }\n",
        "        )\n",
        "\n",
        "        best_model.eval()\n",
        "        all_predictions = []\n",
        "\n",
        "        all_dataset = StockDataset(X, y)\n",
        "        all_loader = DataLoader(all_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, _ in all_loader:\n",
        "                sequences = sequences.to(device)\n",
        "                outputs = best_model(sequences)\n",
        "                all_predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "        all_predictions = target_scaler.inverse_transform(np.array(all_predictions).reshape(-1, 1)).flatten()\n",
        "\n",
        "        prediction_dates = processed_data.index[60:]\n",
        "        individual_iv_predictions[ticker] = pd.Series(all_predictions, index=prediction_dates[:len(all_predictions)])\n",
        "\n",
        "        test_predictions = []\n",
        "        test_actuals = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, targets in test_loader:\n",
        "                sequences, targets = sequences.to(device), targets.to(device)\n",
        "                outputs = best_model(sequences)\n",
        "                test_predictions.extend(outputs.cpu().numpy())\n",
        "                test_actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "        test_predictions = target_scaler.inverse_transform(np.array(test_predictions).reshape(-1, 1)).flatten()\n",
        "        test_actuals = target_scaler.inverse_transform(np.array(test_actuals).reshape(-1, 1)).flatten()\n",
        "\n",
        "        mse = mean_squared_error(test_actuals, test_predictions)\n",
        "        mae = mean_absolute_error(test_actuals, test_predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        print(f\"Stage I Results for {ticker}:\")\n",
        "        print(f\"MSE: {mse:.6f}, MAE: {mae:.6f}, RMSE: {rmse:.6f}\")\n",
        "\n",
        "        plot_stage1_results(train_losses, val_losses, test_actuals, test_predictions, ticker)\n",
        "\n",
        "        stage1_results[ticker] = {\n",
        "            'mse': mse, 'mae': mae, 'rmse': rmse,\n",
        "            'model_path': model_path,\n",
        "            'best_params': best_params,\n",
        "            'scalers': (feature_scaler, target_scaler)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {ticker}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nStage I completed. Successfully processed {len(stage1_results)} stocks.\")\n",
        "print(f\"Individual IV predictions available for: {list(individual_iv_predictions.keys())}\")\n",
        "\n",
        "with open('results/stage1_results.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'stage1_results': stage1_results,\n",
        "        'individual_predictions': individual_iv_predictions,\n",
        "        'stock_returns_data': stock_returns_data\n",
        "    }, f)\n",
        "\n",
        "print(\"Stage I results saved to 'results/stage1_results.pkl'\")"
      ],
      "metadata": {
        "id": "2MwSZjLpOTqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 2 Execution**"
      ],
      "metadata": {
        "id": "ecgbVX4_O3q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 2: Portfolio IV Prediction\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"Loading Stage I results...\")\n",
        "with open('results/stage1_results.pkl', 'rb') as f:\n",
        "    stage1_data = pickle.load(f)\n",
        "\n",
        "stage1_results = stage1_data['stage1_results']\n",
        "individual_iv_predictions = stage1_data['individual_predictions']\n",
        "stock_returns_data = stage1_data['stock_returns_data']\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "os.makedirs('models/stage2', exist_ok=True)\n",
        "\n",
        "portfolio_weights = [1.0/len(individual_iv_predictions)] * len(individual_iv_predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE II: PORTFOLIO IV PREDICTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "stage2_results = {}\n",
        "\n",
        "if len(individual_iv_predictions) >= 2:\n",
        "\n",
        "    if stock_returns_data:\n",
        "        returns_df = pd.DataFrame(stock_returns_data)\n",
        "        returns_df = returns_df.dropna()\n",
        "\n",
        "        correlation_matrices, correlation_dates = calculate_correlation_matrix(returns_df, window=60)\n",
        "        print(f\"Calculated {len(correlation_matrices)} correlation matrices\")\n",
        "\n",
        "    else:\n",
        "        print(\"No stock return data available for correlation calculation. Skipping Stage II.\")\n",
        "        sys.exit()\n",
        "\n",
        "    try:\n",
        "        spy_data = yf.download('SPY', start='2020-01-01', end='2024-01-01')\n",
        "        market_data = pd.DataFrame(index=spy_data.index)\n",
        "        market_data['Market_Vol'] = calculate_realized_volatility(spy_data['Close'])\n",
        "\n",
        "        try:\n",
        "            vix_data = yf.download('^VIX', start='2020-01-01', end='2024-01-01')\n",
        "            market_data['VIX'] = vix_data['Close']\n",
        "        except:\n",
        "            print(\"VIX data not available, using market volatility only\")\n",
        "\n",
        "    except:\n",
        "        market_data = None\n",
        "        print(\"Market data not available\")\n",
        "\n",
        "    print(\"Creating portfolio features...\")\n",
        "    portfolio_features, common_dates = create_portfolio_features(\n",
        "        individual_iv_predictions,\n",
        "        portfolio_weights,\n",
        "        correlation_matrices,\n",
        "        correlation_dates,\n",
        "        market_data\n",
        "    )\n",
        "\n",
        "    print(\"Calculating portfolio IV targets...\")\n",
        "    portfolio_targets, target_dates = calculate_portfolio_iv_target(\n",
        "        individual_iv_predictions,\n",
        "        portfolio_weights,\n",
        "        correlation_matrices,\n",
        "        correlation_dates\n",
        "    )\n",
        "\n",
        "    common_feature_target_dates = pd.Index(common_dates).intersection(target_dates)\n",
        "    common_feature_target_dates = common_feature_target_dates.sort_values()\n",
        "\n",
        "    aligned_portfolio_features = []\n",
        "    aligned_portfolio_targets = []\n",
        "\n",
        "    portfolio_features_df = pd.DataFrame(portfolio_features, index=common_dates)\n",
        "    portfolio_targets_series = pd.Series(portfolio_targets, index=target_dates)\n",
        "\n",
        "    for date in common_feature_target_dates:\n",
        "        aligned_portfolio_features.append(portfolio_features_df.loc[date].values)\n",
        "        aligned_portfolio_targets.append(portfolio_targets_series.loc[date])\n",
        "\n",
        "    aligned_portfolio_features = np.array(aligned_portfolio_features)\n",
        "    aligned_portfolio_targets = np.array(aligned_portfolio_targets)\n",
        "\n",
        "    print(f\"Portfolio dataset size: {len(aligned_portfolio_features)} samples\")\n",
        "    print(f\"Feature dimension: {aligned_portfolio_features.shape[1]}\")\n",
        "\n",
        "    if len(aligned_portfolio_features) == 0:\n",
        "        print(\"No aligned portfolio data available for Stage II. Exiting.\")\n",
        "        sys.exit()\n",
        "\n",
        "    train_size = int(0.7 * len(aligned_portfolio_features))\n",
        "    val_size = int(0.15 * len(aligned_portfolio_features))\n",
        "\n",
        "    X_train_p2 = aligned_portfolio_features[:train_size]\n",
        "    y_train_p2 = aligned_portfolio_targets[:train_size]\n",
        "    X_val_p2 = aligned_portfolio_features[train_size:train_size+val_size]\n",
        "    y_val_p2 = aligned_portfolio_targets[train_size:train_size+val_size]\n",
        "    X_test_p2 = aligned_portfolio_features[train_size+val_size:]\n",
        "    y_test_p2 = aligned_portfolio_targets[train_size+val_size:]\n",
        "\n",
        "    portfolio_scaler = StandardScaler()\n",
        "    X_train_p2_scaled = portfolio_scaler.fit_transform(X_train_p2)\n",
        "    X_val_p2_scaled = portfolio_scaler.transform(X_val_p2)\n",
        "    X_test_p2_scaled = portfolio_scaler.transform(X_test_p2)\n",
        "\n",
        "    print(\"\\nOptimizing MLP hyperparameters...\")\n",
        "    best_mlp_params = optimize_mlp_hyperparameters(\n",
        "        X_train_p2_scaled, y_train_p2, X_val_p2_scaled, y_val_p2, device, n_trials=20\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining optimized MLP Model...\")\n",
        "\n",
        "    hidden_sizes = []\n",
        "    for i in range(best_mlp_params['n_layers']):\n",
        "        hidden_sizes.append(best_mlp_params[f'hidden_size_{i}'])\n",
        "\n",
        "    mlp_model = PortfolioMLPModel(\n",
        "        input_size=X_train_p2_scaled.shape[1],\n",
        "        hidden_sizes=hidden_sizes,\n",
        "        dropout_rate=best_mlp_params['dropout_rate']\n",
        "    ).to(device)\n",
        "\n",
        "    train_dataset_p2 = PortfolioDataset(X_train_p2_scaled, y_train_p2)\n",
        "    val_dataset_p2 = PortfolioDataset(X_val_p2_scaled, y_val_p2)\n",
        "    test_dataset_p2 = PortfolioDataset(X_test_p2_scaled, y_test_p2)\n",
        "\n",
        "    train_loader_p2 = DataLoader(train_dataset_p2, batch_size=best_mlp_params['batch_size'], shuffle=True)\n",
        "    val_loader_p2 = DataLoader(val_dataset_p2, batch_size=best_mlp_params['batch_size'], shuffle=False)\n",
        "    test_loader_p2 = DataLoader(test_dataset_p2, batch_size=best_mlp_params['batch_size'], shuffle=False)\n",
        "\n",
        "    mlp_train_losses, mlp_val_losses, _ = train_portfolio_model(\n",
        "        mlp_model, train_loader_p2, val_loader_p2, 200, best_mlp_params['learning_rate'], device\n",
        "    )\n",
        "\n",
        "    mlp_model_path = 'models/stage2/mlp_best_model.pth'\n",
        "    save_model(\n",
        "        mlp_model,\n",
        "        mlp_model_path,\n",
        "        scalers=portfolio_scaler,\n",
        "        metadata={\n",
        "            'model_type': 'MLP',\n",
        "            'best_params': best_mlp_params,\n",
        "            'input_size': X_train_p2_scaled.shape[1],\n",
        "            'hidden_sizes': hidden_sizes,\n",
        "            'train_size': train_size,\n",
        "            'val_size': val_size,\n",
        "            'test_size': len(X_test_p2)\n",
        "        }\n",
        "    )\n",
        "\n",
        "    mlp_model.eval()\n",
        "    mlp_predictions = []\n",
        "    mlp_actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, targets in test_loader_p2:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "            outputs = mlp_model(features)\n",
        "            mlp_predictions.extend(outputs.cpu().numpy())\n",
        "            mlp_actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "    mlp_predictions = np.array(mlp_predictions).flatten()\n",
        "    mlp_actuals = np.array(mlp_actuals).flatten()\n",
        "\n",
        "    mlp_mse = mean_squared_error(mlp_actuals, mlp_predictions)\n",
        "    mlp_mae = mean_absolute_error(mlp_actuals, mlp_predictions)\n",
        "    mlp_rmse = np.sqrt(mlp_mse)\n",
        "    mlp_corr, _ = pearsonr(mlp_actuals, mlp_predictions)\n",
        "\n",
        "    stage2_results['MLP'] = {\n",
        "        'mse': mlp_mse, 'mae': mlp_mae, 'rmse': mlp_rmse, 'correlation': mlp_corr,\n",
        "        'model_path': mlp_model_path, 'best_params': best_mlp_params\n",
        "    }\n",
        "\n",
        "    print(f\"MLP Results - RMSE: {mlp_rmse:.6f}, MAE: {mlp_mae:.6f}, Correlation: {mlp_corr:.4f}\")\n",
        "\n",
        "    plot_stage2_results(mlp_train_losses, mlp_val_losses, mlp_actuals, mlp_predictions, \"Optimized MLP\")\n",
        "\n",
        "    print(\"\\nStage I (Individual Stock IV) Results:\")\n",
        "    stage1_df = pd.DataFrame({\n",
        "        ticker: {\n",
        "            'RMSE': stage1_results[ticker]['rmse'],\n",
        "            'MAE': stage1_results[ticker]['mae']\n",
        "        } for ticker in stage1_results.keys()\n",
        "    }).T\n",
        "    print(stage1_df)\n",
        "\n",
        "    print(\"\\nStage II (Portfolio IV) Results:\")\n",
        "    stage2_df = pd.DataFrame(stage2_results).T\n",
        "    print(stage2_df)\n",
        "\n",
        "    with open('results/complete_results.pkl', 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'stage1_results': stage1_results,\n",
        "            'stage2_results': stage2_results,\n",
        "            'individual_predictions': individual_iv_predictions,\n",
        "            'portfolio_features': aligned_portfolio_features,\n",
        "            'portfolio_targets': aligned_portfolio_targets,\n",
        "            'portfolio_scaler': portfolio_scaler\n",
        "        }, f)\n",
        "\n",
        "    print(\"\\nComplete results saved to 'results/complete_results.pkl'\")\n",
        "\n",
        "else:\n",
        "    print(\"Not enough individual predictions for Stage II\")"
      ],
      "metadata": {
        "id": "Nvckweq_O7TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27b4eb9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a4cc513"
      },
      "source": [
        "After running the above cell and following the authorization steps, your Google Drive will be mounted at `/content/drive`. You can then save the results file to a specific folder in your Drive, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6833c370"
      },
      "source": [
        "import shutil\n",
        "\n",
        "source_path = 'results/stage1_results.pkl'\n",
        "destination_path = '/content/drive/MyDrive/your_folder_name/stage1_results.pkl'\n",
        "\n",
        "import os\n",
        "os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
        "\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(f\"Results saved to {destination_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "657ba012"
      },
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive already mounted.\")\n",
        "\n",
        "saved_file_path = '/content/drive/MyDrive/your_folder_name/stage1_results.pkl'\n",
        "\n",
        "try:\n",
        "    with open(saved_file_path, 'rb') as f:\n",
        "        loaded_stage1_data = pickle.load(f)\n",
        "\n",
        "    loaded_stage1_results = loaded_stage1_data['stage1_results']\n",
        "    loaded_individual_predictions = loaded_stage1_data['individual_predictions']\n",
        "\n",
        "    print(f\"Successfully loaded data from {saved_file_path}\")\n",
        "    print(f\"Number of stocks in loaded results: {len(loaded_stage1_results)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {saved_file_path}. Please check the folder name and file path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f627b50a"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "saved_file_path = '/content/drive/MyDrive/your_results_folder/two_stage_results.pkl'\n",
        "\n",
        "try:\n",
        "    with open(saved_file_path, 'rb') as f:\n",
        "        loaded_data = pickle.load(f)\n",
        "\n",
        "    loaded_stage1_results = loaded_data['stage1_results']\n",
        "    loaded_stage2_results = loaded_data['stage2_results']\n",
        "    loaded_individual_predictions = loaded_data['individual_predictions']\n",
        "\n",
        "    print(f\"Successfully loaded data from {saved_file_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {saved_file_path}.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzJ1sxYCsyoMozLPDFZy4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}